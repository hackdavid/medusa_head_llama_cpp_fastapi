{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bb640f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
      "Collecting torch\n",
      "  Using cached torch-2.7.0-cp312-cp312-win_amd64.whl.metadata (29 kB)\n",
      "Collecting filelock (from transformers)\n",
      "  Using cached filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.30.0 (from transformers)\n",
      "  Using cached huggingface_hub-0.30.2-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting numpy>=1.17 (from transformers)\n",
      "  Using cached numpy-2.2.5-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\dauddewan\\onedrive - symphonyai\\documents\\learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\.venv\\lib\\site-packages (from transformers) (25.0)\n",
      "Collecting pyyaml>=5.1 (from transformers)\n",
      "  Using cached PyYAML-6.0.2-cp312-cp312-win_amd64.whl.metadata (2.1 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Using cached regex-2024.11.6-cp312-cp312-win_amd64.whl.metadata (41 kB)\n",
      "Collecting requests (from transformers)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Using cached tokenizers-0.21.1-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Using cached safetensors-0.5.3-cp38-abi3-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.30.0->transformers)\n",
      "  Using cached fsspec-2025.3.2-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting typing-extensions>=3.7.4.3 (from huggingface-hub<1.0,>=0.30.0->transformers)\n",
      "  Using cached typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch)\n",
      "  Using cached networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting setuptools (from torch)\n",
      "  Downloading setuptools-80.0.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\dauddewan\\onedrive - symphonyai\\documents\\learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\.venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
      "  Using cached MarkupSafe-3.0.2-cp312-cp312-win_amd64.whl.metadata (4.1 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests->transformers)\n",
      "  Using cached charset_normalizer-3.4.1-cp312-cp312-win_amd64.whl.metadata (36 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->transformers)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->transformers)\n",
      "  Using cached urllib3-2.4.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->transformers)\n",
      "  Using cached certifi-2025.4.26-py3-none-any.whl.metadata (2.5 kB)\n",
      "Using cached transformers-4.51.3-py3-none-any.whl (10.4 MB)\n",
      "Using cached huggingface_hub-0.30.2-py3-none-any.whl (481 kB)\n",
      "Using cached tokenizers-0.21.1-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "Using cached torch-2.7.0-cp312-cp312-win_amd64.whl (212.5 MB)\n",
      "Using cached fsspec-2025.3.2-py3-none-any.whl (194 kB)\n",
      "Using cached numpy-2.2.5-cp312-cp312-win_amd64.whl (12.6 MB)\n",
      "Using cached PyYAML-6.0.2-cp312-cp312-win_amd64.whl (156 kB)\n",
      "Using cached regex-2024.11.6-cp312-cp312-win_amd64.whl (273 kB)\n",
      "Using cached safetensors-0.5.3-cp38-abi3-win_amd64.whl (308 kB)\n",
      "Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "   ---------------------------------------- 0.0/6.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/6.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/6.3 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.5/6.3 MB 1.7 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 1.0/6.3 MB 2.3 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 2.4/6.3 MB 3.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 5.8/6.3 MB 6.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.3/6.3 MB 6.2 MB/s eta 0:00:00\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n",
      "Using cached filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Using cached MarkupSafe-3.0.2-cp312-cp312-win_amd64.whl (15 kB)\n",
      "Using cached networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached charset_normalizer-3.4.1-cp312-cp312-win_amd64.whl (102 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached urllib3-2.4.0-py3-none-any.whl (128 kB)\n",
      "Using cached certifi-2025.4.26-py3-none-any.whl (159 kB)\n",
      "Downloading setuptools-80.0.0-py3-none-any.whl (1.2 MB)\n",
      "   ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.2/1.2 MB 12.5 MB/s eta 0:00:00\n",
      "Installing collected packages: mpmath, urllib3, typing-extensions, tqdm, sympy, setuptools, safetensors, regex, pyyaml, numpy, networkx, MarkupSafe, idna, fsspec, filelock, charset-normalizer, certifi, requests, jinja2, torch, huggingface-hub, tokenizers, transformers\n",
      "\n",
      "   ----------------------------------------  0/23 [mpmath]\n",
      "   ----------------------------------------  0/23 [mpmath]\n",
      "   ----------------------------------------  0/23 [mpmath]\n",
      "   ----------------------------------------  0/23 [mpmath]\n",
      "   ----------------------------------------  0/23 [mpmath]\n",
      "   ----------------------------------------  0/23 [mpmath]\n",
      "   ----------------------------------------  0/23 [mpmath]\n",
      "   - --------------------------------------  1/23 [urllib3]\n",
      "   - --------------------------------------  1/23 [urllib3]\n",
      "   --- ------------------------------------  2/23 [typing-extensions]\n",
      "   ----- ----------------------------------  3/23 [tqdm]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   ------ ---------------------------------  4/23 [sympy]\n",
      "   -------- -------------------------------  5/23 [setuptools]\n",
      "   -------- -------------------------------  5/23 [setuptools]\n",
      "   -------- -------------------------------  5/23 [setuptools]\n",
      "   -------- -------------------------------  5/23 [setuptools]\n",
      "   -------- -------------------------------  5/23 [setuptools]\n",
      "   -------- -------------------------------  5/23 [setuptools]\n",
      "   -------- -------------------------------  5/23 [setuptools]\n",
      "   -------- -------------------------------  5/23 [setuptools]\n",
      "   -------- -------------------------------  5/23 [setuptools]\n",
      "   -------- -------------------------------  5/23 [setuptools]\n",
      "   -------- -------------------------------  5/23 [setuptools]\n",
      "   -------- -------------------------------  5/23 [setuptools]\n",
      "   -------- -------------------------------  5/23 [setuptools]\n",
      "   -------- -------------------------------  5/23 [setuptools]\n",
      "   -------- -------------------------------  5/23 [setuptools]\n",
      "   -------- -------------------------------  5/23 [setuptools]\n",
      "   -------- -------------------------------  5/23 [setuptools]\n",
      "   -------- -------------------------------  5/23 [setuptools]\n",
      "   -------- -------------------------------  5/23 [setuptools]\n",
      "   -------- -------------------------------  5/23 [setuptools]\n",
      "   -------- -------------------------------  5/23 [setuptools]\n",
      "   -------- -------------------------------  5/23 [setuptools]\n",
      "   -------- -------------------------------  5/23 [setuptools]\n",
      "   -------- -------------------------------  5/23 [setuptools]\n",
      "   -------- -------------------------------  5/23 [setuptools]\n",
      "   -------- -------------------------------  5/23 [setuptools]\n",
      "   -------- -------------------------------  5/23 [setuptools]\n",
      "   -------- -------------------------------  5/23 [setuptools]\n",
      "   -------- -------------------------------  5/23 [setuptools]\n",
      "   -------- -------------------------------  5/23 [setuptools]\n",
      "   -------- -------------------------------  5/23 [setuptools]\n",
      "   -------- -------------------------------  5/23 [setuptools]\n",
      "   -------- -------------------------------  5/23 [setuptools]\n",
      "   -------- -------------------------------  5/23 [setuptools]\n",
      "   -------- -------------------------------  5/23 [setuptools]\n",
      "   -------- -------------------------------  5/23 [setuptools]\n",
      "   -------- -------------------------------  5/23 [setuptools]\n",
      "   ---------- -----------------------------  6/23 [safetensors]\n",
      "   ------------ ---------------------------  7/23 [regex]\n",
      "   ------------ ---------------------------  7/23 [regex]\n",
      "   ------------- --------------------------  8/23 [pyyaml]\n",
      "   ------------- --------------------------  8/23 [pyyaml]\n",
      "   --------------- ------------------------  9/23 [numpy]\n",
      "   --------------- ------------------------  9/23 [numpy]\n",
      "   --------------- ------------------------  9/23 [numpy]\n",
      "   --------------- ------------------------  9/23 [numpy]\n",
      "   --------------- ------------------------  9/23 [numpy]\n",
      "   --------------- ------------------------  9/23 [numpy]\n",
      "   --------------- ------------------------  9/23 [numpy]\n",
      "   --------------- ------------------------  9/23 [numpy]\n",
      "   --------------- ------------------------  9/23 [numpy]\n",
      "   --------------- ------------------------  9/23 [numpy]\n",
      "   --------------- ------------------------  9/23 [numpy]\n",
      "   --------------- ------------------------  9/23 [numpy]\n",
      "   --------------- ------------------------  9/23 [numpy]\n",
      "   --------------- ------------------------  9/23 [numpy]\n",
      "   --------------- ------------------------  9/23 [numpy]\n",
      "   --------------- ------------------------  9/23 [numpy]\n",
      "   --------------- ------------------------  9/23 [numpy]\n",
      "   --------------- ------------------------  9/23 [numpy]\n",
      "   --------------- ------------------------  9/23 [numpy]\n",
      "   --------------- ------------------------  9/23 [numpy]\n",
      "   --------------- ------------------------  9/23 [numpy]\n",
      "   --------------- ------------------------  9/23 [numpy]\n",
      "   --------------- ------------------------  9/23 [numpy]\n",
      "   --------------- ------------------------  9/23 [numpy]\n",
      "   --------------- ------------------------  9/23 [numpy]\n",
      "   --------------- ------------------------  9/23 [numpy]\n",
      "   --------------- ------------------------  9/23 [numpy]\n",
      "   --------------- ------------------------  9/23 [numpy]\n",
      "   --------------- ------------------------  9/23 [numpy]\n",
      "   --------------- ------------------------  9/23 [numpy]\n",
      "   --------------- ------------------------  9/23 [numpy]\n",
      "   --------------- ------------------------  9/23 [numpy]\n",
      "   --------------- ------------------------  9/23 [numpy]\n",
      "   --------------- ------------------------  9/23 [numpy]\n",
      "   --------------- ------------------------  9/23 [numpy]\n",
      "   --------------- ------------------------  9/23 [numpy]\n",
      "   --------------- ------------------------  9/23 [numpy]\n",
      "   --------------- ------------------------  9/23 [numpy]\n",
      "   --------------- ------------------------  9/23 [numpy]\n",
      "   --------------- ------------------------  9/23 [numpy]\n",
      "   --------------- ------------------------  9/23 [numpy]\n",
      "   --------------- ------------------------  9/23 [numpy]\n",
      "   --------------- ------------------------  9/23 [numpy]\n",
      "   --------------- ------------------------  9/23 [numpy]\n",
      "   --------------- ------------------------  9/23 [numpy]\n",
      "   --------------- ------------------------  9/23 [numpy]\n",
      "   --------------- ------------------------  9/23 [numpy]\n",
      "   --------------- ------------------------  9/23 [numpy]\n",
      "   --------------- ------------------------  9/23 [numpy]\n",
      "   --------------- ------------------------  9/23 [numpy]\n",
      "   --------------- ------------------------  9/23 [numpy]\n",
      "   --------------- ------------------------  9/23 [numpy]\n",
      "   --------------- ------------------------  9/23 [numpy]\n",
      "   --------------- ------------------------  9/23 [numpy]\n",
      "   --------------- ------------------------  9/23 [numpy]\n",
      "   --------------- ------------------------  9/23 [numpy]\n",
      "   --------------- ------------------------  9/23 [numpy]\n",
      "   --------------- ------------------------  9/23 [numpy]\n",
      "   ----------------- ---------------------- 10/23 [networkx]\n",
      "   ----------------- ---------------------- 10/23 [networkx]\n",
      "   ----------------- ---------------------- 10/23 [networkx]\n",
      "   ----------------- ---------------------- 10/23 [networkx]\n",
      "   ----------------- ---------------------- 10/23 [networkx]\n",
      "   ----------------- ---------------------- 10/23 [networkx]\n",
      "   ----------------- ---------------------- 10/23 [networkx]\n",
      "   ----------------- ---------------------- 10/23 [networkx]\n",
      "   ----------------- ---------------------- 10/23 [networkx]\n",
      "   ----------------- ---------------------- 10/23 [networkx]\n",
      "   ----------------- ---------------------- 10/23 [networkx]\n",
      "   ----------------- ---------------------- 10/23 [networkx]\n",
      "   ----------------- ---------------------- 10/23 [networkx]\n",
      "   ----------------- ---------------------- 10/23 [networkx]\n",
      "   ----------------- ---------------------- 10/23 [networkx]\n",
      "   ----------------- ---------------------- 10/23 [networkx]\n",
      "   ----------------- ---------------------- 10/23 [networkx]\n",
      "   ----------------- ---------------------- 10/23 [networkx]\n",
      "   ----------------- ---------------------- 10/23 [networkx]\n",
      "   ----------------- ---------------------- 10/23 [networkx]\n",
      "   ----------------- ---------------------- 10/23 [networkx]\n",
      "   ----------------- ---------------------- 10/23 [networkx]\n",
      "   ----------------- ---------------------- 10/23 [networkx]\n",
      "   ----------------- ---------------------- 10/23 [networkx]\n",
      "   ----------------- ---------------------- 10/23 [networkx]\n",
      "   ----------------- ---------------------- 10/23 [networkx]\n",
      "   ----------------- ---------------------- 10/23 [networkx]\n",
      "   ----------------- ---------------------- 10/23 [networkx]\n",
      "   ----------------- ---------------------- 10/23 [networkx]\n",
      "   ----------------- ---------------------- 10/23 [networkx]\n",
      "   ----------------- ---------------------- 10/23 [networkx]\n",
      "   ----------------- ---------------------- 10/23 [networkx]\n",
      "   ----------------- ---------------------- 10/23 [networkx]\n",
      "   ----------------- ---------------------- 10/23 [networkx]\n",
      "   ----------------- ---------------------- 10/23 [networkx]\n",
      "   ----------------- ---------------------- 10/23 [networkx]\n",
      "   ----------------- ---------------------- 10/23 [networkx]\n",
      "   ----------------- ---------------------- 10/23 [networkx]\n",
      "   ----------------- ---------------------- 10/23 [networkx]\n",
      "   ----------------- ---------------------- 10/23 [networkx]\n",
      "   ----------------- ---------------------- 10/23 [networkx]\n",
      "   ----------------- ---------------------- 10/23 [networkx]\n",
      "   ----------------- ---------------------- 10/23 [networkx]\n",
      "   ----------------- ---------------------- 10/23 [networkx]\n",
      "   ----------------- ---------------------- 10/23 [networkx]\n",
      "   ----------------- ---------------------- 10/23 [networkx]\n",
      "   ----------------- ---------------------- 10/23 [networkx]\n",
      "   ----------------- ---------------------- 10/23 [networkx]\n",
      "   ----------------- ---------------------- 10/23 [networkx]\n",
      "   ----------------- ---------------------- 10/23 [networkx]\n",
      "   ----------------- ---------------------- 10/23 [networkx]\n",
      "   ----------------- ---------------------- 10/23 [networkx]\n",
      "   ----------------- ---------------------- 10/23 [networkx]\n",
      "   ----------------- ---------------------- 10/23 [networkx]\n",
      "   ----------------- ---------------------- 10/23 [networkx]\n",
      "   ----------------- ---------------------- 10/23 [networkx]\n",
      "   ----------------- ---------------------- 10/23 [networkx]\n",
      "   ----------------- ---------------------- 10/23 [networkx]\n",
      "   ------------------- -------------------- 11/23 [MarkupSafe]\n",
      "   -------------------- ------------------- 12/23 [idna]\n",
      "   -------------------- ------------------- 12/23 [idna]\n",
      "   ---------------------- ----------------- 13/23 [fsspec]\n",
      "   ---------------------- ----------------- 13/23 [fsspec]\n",
      "   ---------------------- ----------------- 13/23 [fsspec]\n",
      "   ---------------------- ----------------- 13/23 [fsspec]\n",
      "   ---------------------- ----------------- 13/23 [fsspec]\n",
      "   ---------------------- ----------------- 13/23 [fsspec]\n",
      "   ---------------------- ----------------- 13/23 [fsspec]\n",
      "   ---------------------- ----------------- 13/23 [fsspec]\n",
      "   ------------------------ --------------- 14/23 [filelock]\n",
      "   ------------------------ --------------- 14/23 [filelock]\n",
      "   -------------------------- ------------- 15/23 [charset-normalizer]\n",
      "   --------------------------- ------------ 16/23 [certifi]\n",
      "   ----------------------------- ---------- 17/23 [requests]\n",
      "   ----------------------------- ---------- 17/23 [requests]\n",
      "   ------------------------------- -------- 18/23 [jinja2]\n",
      "   ------------------------------- -------- 18/23 [jinja2]\n",
      "   ------------------------------- -------- 18/23 [jinja2]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   --------------------------------- ------ 19/23 [torch]\n",
      "   ---------------------------------- ----- 20/23 [huggingface-hub]\n",
      "   ---------------------------------- ----- 20/23 [huggingface-hub]\n",
      "   ---------------------------------- ----- 20/23 [huggingface-hub]\n",
      "   ---------------------------------- ----- 20/23 [huggingface-hub]\n",
      "   ---------------------------------- ----- 20/23 [huggingface-hub]\n",
      "   ---------------------------------- ----- 20/23 [huggingface-hub]\n",
      "   ---------------------------------- ----- 20/23 [huggingface-hub]\n",
      "   ---------------------------------- ----- 20/23 [huggingface-hub]\n",
      "   ---------------------------------- ----- 20/23 [huggingface-hub]\n",
      "   ---------------------------------- ----- 20/23 [huggingface-hub]\n",
      "   ---------------------------------- ----- 20/23 [huggingface-hub]\n",
      "   ---------------------------------- ----- 20/23 [huggingface-hub]\n",
      "   ---------------------------------- ----- 20/23 [huggingface-hub]\n",
      "   ------------------------------------ --- 21/23 [tokenizers]\n",
      "   ------------------------------------ --- 21/23 [tokenizers]\n",
      "   ------------------------------------ --- 21/23 [tokenizers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   -------------------------------------- - 22/23 [transformers]\n",
      "   ---------------------------------------- 23/23 [transformers]\n",
      "\n",
      "Successfully installed MarkupSafe-3.0.2 certifi-2025.4.26 charset-normalizer-3.4.1 filelock-3.18.0 fsspec-2025.3.2 huggingface-hub-0.30.2 idna-3.10 jinja2-3.1.6 mpmath-1.3.0 networkx-3.4.2 numpy-2.2.5 pyyaml-6.0.2 regex-2024.11.6 requests-2.32.3 safetensors-0.5.3 setuptools-80.0.0 sympy-1.14.0 tokenizers-0.21.1 torch-2.7.0 tqdm-4.67.1 transformers-4.51.3 typing-extensions-4.13.2 urllib3-2.4.0\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7cd8e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in c:\\users\\dauddewan\\onedrive - symphonyai\\documents\\learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\.venv\\lib\\site-packages (0.30.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\dauddewan\\onedrive - symphonyai\\documents\\learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\.venv\\lib\\site-packages (from huggingface_hub) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\dauddewan\\onedrive - symphonyai\\documents\\learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\.venv\\lib\\site-packages (from huggingface_hub) (2025.3.2)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\dauddewan\\onedrive - symphonyai\\documents\\learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\.venv\\lib\\site-packages (from huggingface_hub) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\dauddewan\\onedrive - symphonyai\\documents\\learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\.venv\\lib\\site-packages (from huggingface_hub) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\dauddewan\\onedrive - symphonyai\\documents\\learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\.venv\\lib\\site-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\dauddewan\\onedrive - symphonyai\\documents\\learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\.venv\\lib\\site-packages (from huggingface_hub) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\dauddewan\\onedrive - symphonyai\\documents\\learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\.venv\\lib\\site-packages (from huggingface_hub) (4.13.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\dauddewan\\onedrive - symphonyai\\documents\\learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\.venv\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dauddewan\\onedrive - symphonyai\\documents\\learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\.venv\\lib\\site-packages (from requests->huggingface_hub) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dauddewan\\onedrive - symphonyai\\documents\\learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\.venv\\lib\\site-packages (from requests->huggingface_hub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dauddewan\\onedrive - symphonyai\\documents\\learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\.venv\\lib\\site-packages (from requests->huggingface_hub) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dauddewan\\onedrive - symphonyai\\documents\\learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\.venv\\lib\\site-packages (from requests->huggingface_hub) (2025.4.26)\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3851f2ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DaudDewan\\OneDrive - SymphonyAI\\Documents\\Learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\DaudDewan\\OneDrive - SymphonyAI\\Documents\\Learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:933: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\n",
      "For more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n",
      "  warnings.warn(\n",
      "Fetching 10 files: 100%|| 10/10 [18:36<00:00, 111.66s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\DaudDewan\\\\OneDrive - SymphonyAI\\\\Documents\\\\Learning\\\\notebooks\\\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\\\app\\\\vicuna-hf'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "model_id=\"lmsys/vicuna-7b-v1.3\"\n",
    "snapshot_download(repo_id=model_id, local_dir=\"vicuna-hf\",\n",
    "                  local_dir_use_symlinks=False, revision=\"main\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4386dacc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'llama.cpp'...\n",
      "Updating files:  84% (1084/1287)\n",
      "Updating files:  85% (1094/1287)\n",
      "Updating files:  86% (1107/1287)\n",
      "Updating files:  87% (1120/1287)\n",
      "Updating files:  88% (1133/1287)\n",
      "Updating files:  89% (1146/1287)\n",
      "Updating files:  90% (1159/1287)\n",
      "Updating files:  91% (1172/1287)\n",
      "Updating files:  92% (1185/1287)\n",
      "Updating files:  93% (1197/1287)\n",
      "Updating files:  94% (1210/1287)\n",
      "Updating files:  95% (1223/1287)\n",
      "Updating files:  96% (1236/1287)\n",
      "Updating files:  97% (1249/1287)\n",
      "Updating files:  98% (1262/1287)\n",
      "Updating files:  99% (1275/1287)\n",
      "Updating files: 100% (1287/1287)\n",
      "Updating files: 100% (1287/1287), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/ggerganov/llama.cpp.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6ada4cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cpu, https://download.pytorch.org/whl/cpu, https://download.pytorch.org/whl/cpu, https://download.pytorch.org/whl/cpu\n",
      "Requirement already satisfied: numpy in c:\\users\\dauddewan\\onedrive - symphonyai\\documents\\learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\.venv\\lib\\site-packages (from -r c:\\Users\\DaudDewan\\OneDrive - SymphonyAI\\Documents\\Learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\app\\llama.cpp\\requirements\\requirements-convert_legacy_llama.txt (line 1)) (1.26.4)\n",
      "Requirement already satisfied: sentencepiece~=0.2.0 in c:\\users\\dauddewan\\onedrive - symphonyai\\documents\\learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\.venv\\lib\\site-packages (from -r c:\\Users\\DaudDewan\\OneDrive - SymphonyAI\\Documents\\Learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\app\\llama.cpp\\requirements\\requirements-convert_legacy_llama.txt (line 2)) (0.2.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.45.1 in c:\\users\\dauddewan\\onedrive - symphonyai\\documents\\learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\.venv\\lib\\site-packages (from -r c:\\Users\\DaudDewan\\OneDrive - SymphonyAI\\Documents\\Learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\app\\llama.cpp\\requirements\\requirements-convert_legacy_llama.txt (line 3)) (4.46.3)\n",
      "Requirement already satisfied: gguf>=0.1.0 in c:\\users\\dauddewan\\onedrive - symphonyai\\documents\\learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\.venv\\lib\\site-packages (from -r c:\\Users\\DaudDewan\\OneDrive - SymphonyAI\\Documents\\Learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\app\\llama.cpp\\requirements\\requirements-convert_legacy_llama.txt (line 4)) (0.16.2)\n",
      "Requirement already satisfied: protobuf<5.0.0,>=4.21.0 in c:\\users\\dauddewan\\onedrive - symphonyai\\documents\\learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\.venv\\lib\\site-packages (from -r c:\\Users\\DaudDewan\\OneDrive - SymphonyAI\\Documents\\Learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\app\\llama.cpp\\requirements\\requirements-convert_legacy_llama.txt (line 5)) (4.25.7)\n",
      "Requirement already satisfied: torch~=2.2.1 in c:\\users\\dauddewan\\onedrive - symphonyai\\documents\\learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\.venv\\lib\\site-packages (from -r c:\\Users\\DaudDewan\\OneDrive - SymphonyAI\\Documents\\Learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\app\\llama.cpp\\requirements\\requirements-convert_hf_to_gguf.txt (line 3)) (2.2.2+cpu)\n",
      "Requirement already satisfied: aiohttp~=3.9.3 in c:\\users\\dauddewan\\onedrive - symphonyai\\documents\\learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\.venv\\lib\\site-packages (from -r c:\\Users\\DaudDewan\\OneDrive - SymphonyAI\\Documents\\Learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\app\\llama.cpp\\requirements\\requirements-tool_bench.txt (line 1)) (3.9.5)\n",
      "Requirement already satisfied: pytest~=8.3.3 in c:\\users\\dauddewan\\onedrive - symphonyai\\documents\\learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\.venv\\lib\\site-packages (from -r c:\\Users\\DaudDewan\\OneDrive - SymphonyAI\\Documents\\Learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\app\\llama.cpp\\requirements\\requirements-tool_bench.txt (line 2)) (8.3.5)\n",
      "Requirement already satisfied: huggingface_hub~=0.23.2 in c:\\users\\dauddewan\\onedrive - symphonyai\\documents\\learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\.venv\\lib\\site-packages (from -r c:\\Users\\DaudDewan\\OneDrive - SymphonyAI\\Documents\\Learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\app\\llama.cpp\\requirements\\requirements-tool_bench.txt (line 3)) (0.23.5)\n",
      "Requirement already satisfied: matplotlib~=3.10.0 in c:\\users\\dauddewan\\onedrive - symphonyai\\documents\\learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\.venv\\lib\\site-packages (from -r c:\\Users\\DaudDewan\\OneDrive - SymphonyAI\\Documents\\Learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\app\\llama.cpp\\requirements\\requirements-tool_bench.txt (line 4)) (3.10.1)\n",
      "Requirement already satisfied: openai~=1.55.3 in c:\\users\\dauddewan\\onedrive - symphonyai\\documents\\learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\.venv\\lib\\site-packages (from -r c:\\Users\\DaudDewan\\OneDrive - SymphonyAI\\Documents\\Learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\app\\llama.cpp\\requirements\\requirements-tool_bench.txt (line 6)) (1.55.3)\n",
      "Requirement already satisfied: pandas~=2.2.3 in c:\\users\\dauddewan\\onedrive - symphonyai\\documents\\learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\.venv\\lib\\site-packages (from -r c:\\Users\\DaudDewan\\OneDrive - SymphonyAI\\Documents\\Learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\app\\llama.cpp\\requirements\\requirements-tool_bench.txt (line 7)) (2.2.3)\n",
      "Requirement already satisfied: requests~=2.32.3 in c:\\users\\dauddewan\\onedrive - symphonyai\\documents\\learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\.venv\\lib\\site-packages (from -r c:\\Users\\DaudDewan\\OneDrive - SymphonyAI\\Documents\\Learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\app\\llama.cpp\\requirements\\requirements-tool_bench.txt (line 8)) (2.32.3)\n",
      "Requirement already satisfied: wget~=3.2 in c:\\users\\dauddewan\\onedrive - symphonyai\\documents\\learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\.venv\\lib\\site-packages (from -r c:\\Users\\DaudDewan\\OneDrive - SymphonyAI\\Documents\\Learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\app\\llama.cpp\\requirements\\requirements-tool_bench.txt (line 9)) (3.2)\n",
      "Requirement already satisfied: typer~=0.15.1 in c:\\users\\dauddewan\\onedrive - symphonyai\\documents\\learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\.venv\\lib\\site-packages (from -r c:\\Users\\DaudDewan\\OneDrive - SymphonyAI\\Documents\\Learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\app\\llama.cpp\\requirements\\requirements-tool_bench.txt (line 10)) (0.15.2)\n",
      "Requirement already satisfied: seaborn~=0.13.2 in c:\\users\\dauddewan\\onedrive - symphonyai\\documents\\learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\.venv\\lib\\site-packages (from -r c:\\Users\\DaudDewan\\OneDrive - SymphonyAI\\Documents\\Learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\app\\llama.cpp\\requirements\\requirements-tool_bench.txt (line 11)) (0.13.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\dauddewan\\onedrive - symphonyai\\documents\\learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.45.1->-r c:\\Users\\DaudDewan\\OneDrive - SymphonyAI\\Documents\\Learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\app\\llama.cpp\\requirements\\requirements-convert_legacy_llama.txt (line 3)) (3.18.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\dauddewan\\onedrive - symphonyai\\documents\\learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.45.1->-r c:\\Users\\DaudDewan\\OneDrive - SymphonyAI\\Documents\\Learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\app\\llama.cpp\\requirements\\requirements-convert_legacy_llama.txt (line 3)) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\dauddewan\\onedrive - symphonyai\\documents\\learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.45.1->-r c:\\Users\\DaudDewan\\OneDrive - SymphonyAI\\Documents\\Learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\app\\llama.cpp\\requirements\\requirements-convert_legacy_llama.txt (line 3)) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\dauddewan\\onedrive - symphonyai\\documents\\learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.45.1->-r c:\\Users\\DaudDewan\\OneDrive - SymphonyAI\\Documents\\Learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\app\\llama.cpp\\requirements\\requirements-convert_legacy_llama.txt (line 3)) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\dauddewan\\onedrive - symphonyai\\documents\\learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.45.1->-r c:\\Users\\DaudDewan\\OneDrive - SymphonyAI\\Documents\\Learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\app\\llama.cpp\\requirements\\requirements-convert_legacy_llama.txt (line 3)) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\dauddewan\\onedrive - symphonyai\\documents\\learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.45.1->-r c:\\Users\\DaudDewan\\OneDrive - SymphonyAI\\Documents\\Learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\app\\llama.cpp\\requirements\\requirements-convert_legacy_llama.txt (line 3)) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\dauddewan\\onedrive - symphonyai\\documents\\learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.45.1->-r c:\\Users\\DaudDewan\\OneDrive - SymphonyAI\\Documents\\Learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\app\\llama.cpp\\requirements\\requirements-convert_legacy_llama.txt (line 3)) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\dauddewan\\onedrive - symphonyai\\documents\\learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\.venv\\lib\\site-packages (from torch~=2.2.1->-r c:\\Users\\DaudDewan\\OneDrive - SymphonyAI\\Documents\\Learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\app\\llama.cpp\\requirements\\requirements-convert_hf_to_gguf.txt (line 3)) (4.13.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\dauddewan\\onedrive - symphonyai\\documents\\learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\.venv\\lib\\site-packages (from torch~=2.2.1->-r c:\\Users\\DaudDewan\\OneDrive - SymphonyAI\\Documents\\Learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\app\\llama.cpp\\requirements\\requirements-convert_hf_to_gguf.txt (line 3)) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\dauddewan\\onedrive - symphonyai\\documents\\learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\.venv\\lib\\site-packages (from torch~=2.2.1->-r c:\\Users\\DaudDewan\\OneDrive - SymphonyAI\\Documents\\Learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\app\\llama.cpp\\requirements\\requirements-convert_hf_to_gguf.txt (line 3)) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\dauddewan\\onedrive - symphonyai\\documents\\learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\.venv\\lib\\site-packages (from torch~=2.2.1->-r c:\\Users\\DaudDewan\\OneDrive - SymphonyAI\\Documents\\Learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\app\\llama.cpp\\requirements\\requirements-convert_hf_to_gguf.txt (line 3)) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\dauddewan\\onedrive - symphonyai\\documents\\learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\.venv\\lib\\site-packages (from torch~=2.2.1->-r c:\\Users\\DaudDewan\\OneDrive - SymphonyAI\\Documents\\Learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\app\\llama.cpp\\requirements\\requirements-convert_hf_to_gguf.txt (line 3)) (2025.3.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\dauddewan\\onedrive - symphonyai\\documents\\learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\.venv\\lib\\site-packages (from aiohttp~=3.9.3->-r c:\\Users\\DaudDewan\\OneDrive - SymphonyAI\\Documents\\Learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\app\\llama.cpp\\requirements\\requirements-tool_bench.txt (line 1)) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\dauddewan\\onedrive - symphonyai\\documents\\learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\.venv\\lib\\site-packages (from aiohttp~=3.9.3->-r c:\\Users\\DaudDewan\\OneDrive - SymphonyAI\\Documents\\Learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\app\\llama.cpp\\requirements\\requirements-tool_bench.txt (line 1)) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\dauddewan\\onedrive - symphonyai\\documents\\learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\.venv\\lib\\site-packages (from aiohttp~=3.9.3->-r c:\\Users\\DaudDewan\\OneDrive - SymphonyAI\\Documents\\Learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\app\\llama.cpp\\requirements\\requirements-tool_bench.txt (line 1)) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\dauddewan\\onedrive - symphonyai\\documents\\learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\.venv\\lib\\site-packages (from aiohttp~=3.9.3->-r c:\\Users\\DaudDewan\\OneDrive - SymphonyAI\\Documents\\Learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\app\\llama.cpp\\requirements\\requirements-tool_bench.txt (line 1)) (6.4.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\dauddewan\\onedrive - symphonyai\\documents\\learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\.venv\\lib\\site-packages (from aiohttp~=3.9.3->-r c:\\Users\\DaudDewan\\OneDrive - SymphonyAI\\Documents\\Learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\app\\llama.cpp\\requirements\\requirements-tool_bench.txt (line 1)) (1.20.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\dauddewan\\onedrive - symphonyai\\documents\\learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\.venv\\lib\\site-packages (from pytest~=8.3.3->-r c:\\Users\\DaudDewan\\OneDrive - SymphonyAI\\Documents\\Learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\app\\llama.cpp\\requirements\\requirements-tool_bench.txt (line 2)) (0.4.6)\n",
      "Requirement already satisfied: iniconfig in c:\\users\\dauddewan\\onedrive - symphonyai\\documents\\learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\.venv\\lib\\site-packages (from pytest~=8.3.3->-r c:\\Users\\DaudDewan\\OneDrive - SymphonyAI\\Documents\\Learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\app\\llama.cpp\\requirements\\requirements-tool_bench.txt (line 2)) (2.1.0)\n",
      "Requirement already satisfied: pluggy<2,>=1.5 in c:\\users\\dauddewan\\onedrive - symphonyai\\documents\\learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\.venv\\lib\\site-packages (from pytest~=8.3.3->-r c:\\Users\\DaudDewan\\OneDrive - SymphonyAI\\Documents\\Learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\app\\llama.cpp\\requirements\\requirements-tool_bench.txt (line 2)) (1.5.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\dauddewan\\onedrive - symphonyai\\documents\\learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\.venv\\lib\\site-packages (from matplotlib~=3.10.0->-r c:\\Users\\DaudDewan\\OneDrive - SymphonyAI\\Documents\\Learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\app\\llama.cpp\\requirements\\requirements-tool_bench.txt (line 4)) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\dauddewan\\onedrive - symphonyai\\documents\\learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\.venv\\lib\\site-packages (from matplotlib~=3.10.0->-r c:\\Users\\DaudDewan\\OneDrive - SymphonyAI\\Documents\\Learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\app\\llama.cpp\\requirements\\requirements-tool_bench.txt (line 4)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\dauddewan\\onedrive - symphonyai\\documents\\learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\.venv\\lib\\site-packages (from matplotlib~=3.10.0->-r c:\\Users\\DaudDewan\\OneDrive - SymphonyAI\\Documents\\Learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\app\\llama.cpp\\requirements\\requirements-tool_bench.txt (line 4)) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\dauddewan\\onedrive - symphonyai\\documents\\learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\.venv\\lib\\site-packages (from matplotlib~=3.10.0->-r c:\\Users\\DaudDewan\\OneDrive - SymphonyAI\\Documents\\Learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\app\\llama.cpp\\requirements\\requirements-tool_bench.txt (line 4)) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\dauddewan\\onedrive - symphonyai\\documents\\learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\.venv\\lib\\site-packages (from matplotlib~=3.10.0->-r c:\\Users\\DaudDewan\\OneDrive - SymphonyAI\\Documents\\Learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\app\\llama.cpp\\requirements\\requirements-tool_bench.txt (line 4)) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\dauddewan\\onedrive - symphonyai\\documents\\learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\.venv\\lib\\site-packages (from matplotlib~=3.10.0->-r c:\\Users\\DaudDewan\\OneDrive - SymphonyAI\\Documents\\Learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\app\\llama.cpp\\requirements\\requirements-tool_bench.txt (line 4)) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\dauddewan\\onedrive - symphonyai\\documents\\learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\.venv\\lib\\site-packages (from matplotlib~=3.10.0->-r c:\\Users\\DaudDewan\\OneDrive - SymphonyAI\\Documents\\Learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\app\\llama.cpp\\requirements\\requirements-tool_bench.txt (line 4)) (2.9.0.post0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\dauddewan\\onedrive - symphonyai\\documents\\learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\.venv\\lib\\site-packages (from openai~=1.55.3->-r c:\\Users\\DaudDewan\\OneDrive - SymphonyAI\\Documents\\Learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\app\\llama.cpp\\requirements\\requirements-tool_bench.txt (line 6)) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\dauddewan\\onedrive - symphonyai\\documents\\learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\.venv\\lib\\site-packages (from openai~=1.55.3->-r c:\\Users\\DaudDewan\\OneDrive - SymphonyAI\\Documents\\Learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\app\\llama.cpp\\requirements\\requirements-tool_bench.txt (line 6)) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\dauddewan\\onedrive - symphonyai\\documents\\learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\.venv\\lib\\site-packages (from openai~=1.55.3->-r c:\\Users\\DaudDewan\\OneDrive - SymphonyAI\\Documents\\Learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\app\\llama.cpp\\requirements\\requirements-tool_bench.txt (line 6)) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\dauddewan\\onedrive - symphonyai\\documents\\learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\.venv\\lib\\site-packages (from openai~=1.55.3->-r c:\\Users\\DaudDewan\\OneDrive - SymphonyAI\\Documents\\Learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\app\\llama.cpp\\requirements\\requirements-tool_bench.txt (line 6)) (0.9.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\dauddewan\\onedrive - symphonyai\\documents\\learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\.venv\\lib\\site-packages (from openai~=1.55.3->-r c:\\Users\\DaudDewan\\OneDrive - SymphonyAI\\Documents\\Learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\app\\llama.cpp\\requirements\\requirements-tool_bench.txt (line 6)) (2.11.3)\n",
      "Requirement already satisfied: sniffio in c:\\users\\dauddewan\\onedrive - symphonyai\\documents\\learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\.venv\\lib\\site-packages (from openai~=1.55.3->-r c:\\Users\\DaudDewan\\OneDrive - SymphonyAI\\Documents\\Learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\app\\llama.cpp\\requirements\\requirements-tool_bench.txt (line 6)) (1.3.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\dauddewan\\onedrive - symphonyai\\documents\\learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\.venv\\lib\\site-packages (from pandas~=2.2.3->-r c:\\Users\\DaudDewan\\OneDrive - SymphonyAI\\Documents\\Learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\app\\llama.cpp\\requirements\\requirements-tool_bench.txt (line 7)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\dauddewan\\onedrive - symphonyai\\documents\\learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\.venv\\lib\\site-packages (from pandas~=2.2.3->-r c:\\Users\\DaudDewan\\OneDrive - SymphonyAI\\Documents\\Learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\app\\llama.cpp\\requirements\\requirements-tool_bench.txt (line 7)) (2025.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dauddewan\\onedrive - symphonyai\\documents\\learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\.venv\\lib\\site-packages (from requests~=2.32.3->-r c:\\Users\\DaudDewan\\OneDrive - SymphonyAI\\Documents\\Learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\app\\llama.cpp\\requirements\\requirements-tool_bench.txt (line 8)) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dauddewan\\onedrive - symphonyai\\documents\\learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\.venv\\lib\\site-packages (from requests~=2.32.3->-r c:\\Users\\DaudDewan\\OneDrive - SymphonyAI\\Documents\\Learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\app\\llama.cpp\\requirements\\requirements-tool_bench.txt (line 8)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dauddewan\\onedrive - symphonyai\\documents\\learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\.venv\\lib\\site-packages (from requests~=2.32.3->-r c:\\Users\\DaudDewan\\OneDrive - SymphonyAI\\Documents\\Learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\app\\llama.cpp\\requirements\\requirements-tool_bench.txt (line 8)) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dauddewan\\onedrive - symphonyai\\documents\\learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\.venv\\lib\\site-packages (from requests~=2.32.3->-r c:\\Users\\DaudDewan\\OneDrive - SymphonyAI\\Documents\\Learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\app\\llama.cpp\\requirements\\requirements-tool_bench.txt (line 8)) (2025.4.26)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\dauddewan\\onedrive - symphonyai\\documents\\learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\.venv\\lib\\site-packages (from typer~=0.15.1->-r c:\\Users\\DaudDewan\\OneDrive - SymphonyAI\\Documents\\Learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\app\\llama.cpp\\requirements\\requirements-tool_bench.txt (line 10)) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\dauddewan\\onedrive - symphonyai\\documents\\learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\.venv\\lib\\site-packages (from typer~=0.15.1->-r c:\\Users\\DaudDewan\\OneDrive - SymphonyAI\\Documents\\Learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\app\\llama.cpp\\requirements\\requirements-tool_bench.txt (line 10)) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\dauddewan\\onedrive - symphonyai\\documents\\learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\.venv\\lib\\site-packages (from typer~=0.15.1->-r c:\\Users\\DaudDewan\\OneDrive - SymphonyAI\\Documents\\Learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\app\\llama.cpp\\requirements\\requirements-tool_bench.txt (line 10)) (14.0.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\dauddewan\\onedrive - symphonyai\\documents\\learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai~=1.55.3->-r c:\\Users\\DaudDewan\\OneDrive - SymphonyAI\\Documents\\Learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\app\\llama.cpp\\requirements\\requirements-tool_bench.txt (line 6)) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\dauddewan\\onedrive - symphonyai\\documents\\learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai~=1.55.3->-r c:\\Users\\DaudDewan\\OneDrive - SymphonyAI\\Documents\\Learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\app\\llama.cpp\\requirements\\requirements-tool_bench.txt (line 6)) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\dauddewan\\onedrive - symphonyai\\documents\\learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\.venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai~=1.55.3->-r c:\\Users\\DaudDewan\\OneDrive - SymphonyAI\\Documents\\Learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\app\\llama.cpp\\requirements\\requirements-tool_bench.txt (line 6)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in c:\\users\\dauddewan\\onedrive - symphonyai\\documents\\learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\.venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai~=1.55.3->-r c:\\Users\\DaudDewan\\OneDrive - SymphonyAI\\Documents\\Learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\app\\llama.cpp\\requirements\\requirements-tool_bench.txt (line 6)) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\dauddewan\\onedrive - symphonyai\\documents\\learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\.venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai~=1.55.3->-r c:\\Users\\DaudDewan\\OneDrive - SymphonyAI\\Documents\\Learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\app\\llama.cpp\\requirements\\requirements-tool_bench.txt (line 6)) (0.4.0)\n",
      "Requirement already satisfied: propcache>=0.2.1 in c:\\users\\dauddewan\\onedrive - symphonyai\\documents\\learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\.venv\\lib\\site-packages (from yarl<2.0,>=1.0->aiohttp~=3.9.3->-r c:\\Users\\DaudDewan\\OneDrive - SymphonyAI\\Documents\\Learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\app\\llama.cpp\\requirements\\requirements-tool_bench.txt (line 1)) (0.3.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\dauddewan\\onedrive - symphonyai\\documents\\learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\.venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib~=3.10.0->-r c:\\Users\\DaudDewan\\OneDrive - SymphonyAI\\Documents\\Learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\app\\llama.cpp\\requirements\\requirements-tool_bench.txt (line 4)) (1.17.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\dauddewan\\onedrive - symphonyai\\documents\\learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\.venv\\lib\\site-packages (from rich>=10.11.0->typer~=0.15.1->-r c:\\Users\\DaudDewan\\OneDrive - SymphonyAI\\Documents\\Learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\app\\llama.cpp\\requirements\\requirements-tool_bench.txt (line 10)) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\dauddewan\\onedrive - symphonyai\\documents\\learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\.venv\\lib\\site-packages (from rich>=10.11.0->typer~=0.15.1->-r c:\\Users\\DaudDewan\\OneDrive - SymphonyAI\\Documents\\Learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\app\\llama.cpp\\requirements\\requirements-tool_bench.txt (line 10)) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\dauddewan\\onedrive - symphonyai\\documents\\learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\.venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer~=0.15.1->-r c:\\Users\\DaudDewan\\OneDrive - SymphonyAI\\Documents\\Learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\app\\llama.cpp\\requirements\\requirements-tool_bench.txt (line 10)) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\dauddewan\\onedrive - symphonyai\\documents\\learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\.venv\\lib\\site-packages (from jinja2->torch~=2.2.1->-r c:\\Users\\DaudDewan\\OneDrive - SymphonyAI\\Documents\\Learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\app\\llama.cpp\\requirements\\requirements-convert_hf_to_gguf.txt (line 3)) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\dauddewan\\onedrive - symphonyai\\documents\\learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\.venv\\lib\\site-packages (from sympy->torch~=2.2.1->-r c:\\Users\\DaudDewan\\OneDrive - SymphonyAI\\Documents\\Learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\app\\llama.cpp\\requirements\\requirements-convert_hf_to_gguf.txt (line 3)) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install -r llama.cpp/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ad73f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: convert_hf_to_gguf.py [-h] [--vocab-only] [--outfile OUTFILE]\n",
      "                             [--outtype {f32,f16,bf16,q8_0,tq1_0,tq2_0,auto}]\n",
      "                             [--bigendian] [--use-temp-file] [--no-lazy]\n",
      "                             [--model-name MODEL_NAME] [--verbose]\n",
      "                             [--split-max-tensors SPLIT_MAX_TENSORS]\n",
      "                             [--split-max-size SPLIT_MAX_SIZE] [--dry-run]\n",
      "                             [--no-tensor-first-split] [--metadata METADATA]\n",
      "                             [--print-supported-models] [--remote] [--mmproj]\n",
      "                             [model]\n",
      "\n",
      "Convert a huggingface model to a GGML compatible file\n",
      "\n",
      "positional arguments:\n",
      "  model                 directory containing model file\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n",
      "  --vocab-only          extract only the vocab\n",
      "  --outfile OUTFILE     path to write to; default: based on input. {ftype}\n",
      "                        will be replaced by the outtype.\n",
      "  --outtype {f32,f16,bf16,q8_0,tq1_0,tq2_0,auto}\n",
      "                        output format - use f32 for float32, f16 for float16,\n",
      "                        bf16 for bfloat16, q8_0 for Q8_0, tq1_0 or tq2_0 for\n",
      "                        ternary, and auto for the highest-fidelity 16-bit\n",
      "                        float type depending on the first loaded tensor type\n",
      "  --bigendian           model is executed on big endian machine\n",
      "  --use-temp-file       use the tempfile library while processing (helpful\n",
      "                        when running out of memory, process killed)\n",
      "  --no-lazy             use more RAM by computing all outputs before writing\n",
      "                        (use in case lazy evaluation is broken)\n",
      "  --model-name MODEL_NAME\n",
      "                        name of the model\n",
      "  --verbose             increase output verbosity\n",
      "  --split-max-tensors SPLIT_MAX_TENSORS\n",
      "                        max tensors in each split\n",
      "  --split-max-size SPLIT_MAX_SIZE\n",
      "                        max size per split N(M|G)\n",
      "  --dry-run             only print out a split plan and exit, without writing\n",
      "                        any new files\n",
      "  --no-tensor-first-split\n",
      "                        do not add tensors to the first split (disabled by\n",
      "                        default)\n",
      "  --metadata METADATA   Specify the path for an authorship metadata override\n",
      "                        file\n",
      "  --print-supported-models\n",
      "                        Print the supported models\n",
      "  --remote              (Experimental) Read safetensors file remotely without\n",
      "                        downloading to disk. Config and tokenizer files will\n",
      "                        still be downloaded. To use this feature, you need to\n",
      "                        specify Hugging Face model repo name instead of a\n",
      "                        local directory. For example:\n",
      "                        'HuggingFaceTB/SmolLM2-1.7B-Instruct'. Note: To access\n",
      "                        gated repo, set HF_TOKEN environment variable to your\n",
      "                        Hugging Face token.\n",
      "  --mmproj              (Experimental) Export multimodal projector (mmproj)\n",
      "                        for vision models. This will only work on some vision\n",
      "                        models. A prefix 'mmproj-' will be added to the output\n",
      "                        file name.\n"
     ]
    }
   ],
   "source": [
    "!python llama.cpp/convert_hf_to_gguf.py -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ca6102a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hf-to-gguf:Loading model: vicuna-hf\n",
      "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
      "INFO:hf-to-gguf:Exporting model...\n",
      "INFO:hf-to-gguf:gguf: loading model weight map from 'pytorch_model.bin.index.json'\n",
      "INFO:hf-to-gguf:gguf: loading model part 'pytorch_model-00001-of-00002.bin'\n",
      "INFO:hf-to-gguf:token_embd.weight,           torch.float16 --> Q8_0, shape = {4096, 32000}\n",
      "INFO:hf-to-gguf:blk.0.attn_q.weight,         torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.0.attn_k.weight,         torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.0.attn_v.weight,         torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.0.attn_output.weight,    torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.0.ffn_gate.weight,       torch.float16 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.0.ffn_down.weight,       torch.float16 --> Q8_0, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.0.ffn_up.weight,         torch.float16 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.0.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.0.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.1.attn_q.weight,         torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.1.attn_k.weight,         torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.1.attn_v.weight,         torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.1.attn_output.weight,    torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.1.ffn_gate.weight,       torch.float16 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.1.ffn_down.weight,       torch.float16 --> Q8_0, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.1.ffn_up.weight,         torch.float16 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.1.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.1.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.2.attn_q.weight,         torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.2.attn_k.weight,         torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.2.attn_v.weight,         torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.2.attn_output.weight,    torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.2.ffn_gate.weight,       torch.float16 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.2.ffn_down.weight,       torch.float16 --> Q8_0, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.2.ffn_up.weight,         torch.float16 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.2.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.2.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.3.attn_q.weight,         torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.3.attn_k.weight,         torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.3.attn_v.weight,         torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.3.attn_output.weight,    torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.3.ffn_gate.weight,       torch.float16 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.3.ffn_down.weight,       torch.float16 --> Q8_0, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.3.ffn_up.weight,         torch.float16 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.3.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.3.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.4.attn_q.weight,         torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.4.attn_k.weight,         torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.4.attn_v.weight,         torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.4.attn_output.weight,    torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.4.ffn_gate.weight,       torch.float16 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.4.ffn_down.weight,       torch.float16 --> Q8_0, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.4.ffn_up.weight,         torch.float16 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.4.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.4.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.5.attn_q.weight,         torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.5.attn_k.weight,         torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.5.attn_v.weight,         torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.5.attn_output.weight,    torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.5.ffn_gate.weight,       torch.float16 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.5.ffn_down.weight,       torch.float16 --> Q8_0, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.5.ffn_up.weight,         torch.float16 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.5.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.5.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.6.attn_q.weight,         torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.6.attn_k.weight,         torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.6.attn_v.weight,         torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.6.attn_output.weight,    torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.6.ffn_gate.weight,       torch.float16 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.6.ffn_down.weight,       torch.float16 --> Q8_0, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.6.ffn_up.weight,         torch.float16 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.6.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.6.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.7.attn_q.weight,         torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.7.attn_k.weight,         torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.7.attn_v.weight,         torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.7.attn_output.weight,    torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.7.ffn_gate.weight,       torch.float16 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.7.ffn_down.weight,       torch.float16 --> Q8_0, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.7.ffn_up.weight,         torch.float16 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.7.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.7.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.8.attn_q.weight,         torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.8.attn_k.weight,         torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.8.attn_v.weight,         torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.8.attn_output.weight,    torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.8.ffn_gate.weight,       torch.float16 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.8.ffn_down.weight,       torch.float16 --> Q8_0, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.8.ffn_up.weight,         torch.float16 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.8.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.8.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.9.attn_q.weight,         torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.9.attn_k.weight,         torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.9.attn_v.weight,         torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.9.attn_output.weight,    torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.9.ffn_gate.weight,       torch.float16 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.9.ffn_down.weight,       torch.float16 --> Q8_0, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.9.ffn_up.weight,         torch.float16 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.9.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.9.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.10.attn_q.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.10.attn_k.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.10.attn_v.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.10.attn_output.weight,   torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.10.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.10.ffn_down.weight,      torch.float16 --> Q8_0, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.10.ffn_up.weight,        torch.float16 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.10.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.10.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.11.attn_q.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.11.attn_k.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.11.attn_v.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.11.attn_output.weight,   torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.11.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.11.ffn_down.weight,      torch.float16 --> Q8_0, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.11.ffn_up.weight,        torch.float16 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.11.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.11.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.12.attn_q.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.12.attn_k.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.12.attn_v.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.12.attn_output.weight,   torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.12.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.12.ffn_down.weight,      torch.float16 --> Q8_0, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.12.ffn_up.weight,        torch.float16 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.12.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.12.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.13.attn_q.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.13.attn_k.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.13.attn_v.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.13.attn_output.weight,   torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.13.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.13.ffn_down.weight,      torch.float16 --> Q8_0, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.13.ffn_up.weight,        torch.float16 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.13.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.13.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.14.attn_q.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.14.attn_k.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.14.attn_v.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.14.attn_output.weight,   torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.14.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.14.ffn_down.weight,      torch.float16 --> Q8_0, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.14.ffn_up.weight,        torch.float16 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.14.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.14.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.15.attn_q.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.15.attn_k.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.15.attn_v.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.15.attn_output.weight,   torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.15.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.15.ffn_down.weight,      torch.float16 --> Q8_0, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.15.ffn_up.weight,        torch.float16 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.15.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.15.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.16.attn_q.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.16.attn_k.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.16.attn_v.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.16.attn_output.weight,   torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.16.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.16.ffn_down.weight,      torch.float16 --> Q8_0, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.16.ffn_up.weight,        torch.float16 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.16.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.16.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.17.attn_q.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.17.attn_k.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.17.attn_v.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.17.attn_output.weight,   torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.17.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.17.ffn_down.weight,      torch.float16 --> Q8_0, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.17.ffn_up.weight,        torch.float16 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.17.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.17.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.18.attn_q.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.18.attn_k.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.18.attn_v.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.18.attn_output.weight,   torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.18.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.18.ffn_down.weight,      torch.float16 --> Q8_0, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.18.ffn_up.weight,        torch.float16 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.18.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.18.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.19.attn_q.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.19.attn_k.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.19.attn_v.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.19.attn_output.weight,   torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.19.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.19.ffn_down.weight,      torch.float16 --> Q8_0, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.19.ffn_up.weight,        torch.float16 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.19.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.19.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.20.attn_q.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.20.attn_k.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.20.attn_v.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.20.attn_output.weight,   torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.20.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.20.ffn_down.weight,      torch.float16 --> Q8_0, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.20.ffn_up.weight,        torch.float16 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.20.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.20.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.21.attn_q.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.21.attn_k.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.21.attn_v.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.21.attn_output.weight,   torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.21.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.21.ffn_down.weight,      torch.float16 --> Q8_0, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.21.ffn_up.weight,        torch.float16 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.21.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.21.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.22.attn_q.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.22.attn_k.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.22.attn_v.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.22.attn_output.weight,   torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.22.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.22.ffn_down.weight,      torch.float16 --> Q8_0, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.22.ffn_up.weight,        torch.float16 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.22.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.22.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.23.attn_q.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.23.attn_k.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.23.attn_v.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.23.attn_output.weight,   torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.23.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.23.ffn_down.weight,      torch.float16 --> Q8_0, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.23.ffn_up.weight,        torch.float16 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.23.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.23.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'pytorch_model-00002-of-00002.bin'\n",
      "INFO:hf-to-gguf:blk.24.attn_q.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.24.attn_k.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.24.attn_v.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.24.attn_output.weight,   torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.24.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.24.ffn_down.weight,      torch.float16 --> Q8_0, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.24.ffn_up.weight,        torch.float16 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.24.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.24.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.25.attn_q.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.25.attn_k.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.25.attn_v.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.25.attn_output.weight,   torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.25.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.25.ffn_down.weight,      torch.float16 --> Q8_0, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.25.ffn_up.weight,        torch.float16 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.25.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.25.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.26.attn_q.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.26.attn_k.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.26.attn_v.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.26.attn_output.weight,   torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.26.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.26.ffn_down.weight,      torch.float16 --> Q8_0, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.26.ffn_up.weight,        torch.float16 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.26.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.26.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.27.attn_q.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.27.attn_k.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.27.attn_v.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.27.attn_output.weight,   torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.27.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.27.ffn_down.weight,      torch.float16 --> Q8_0, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.27.ffn_up.weight,        torch.float16 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.27.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.27.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.28.attn_q.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.28.attn_k.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.28.attn_v.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.28.attn_output.weight,   torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.28.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.28.ffn_down.weight,      torch.float16 --> Q8_0, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.28.ffn_up.weight,        torch.float16 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.28.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.28.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.29.attn_q.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.29.attn_k.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.29.attn_v.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.29.attn_output.weight,   torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.29.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.29.ffn_down.weight,      torch.float16 --> Q8_0, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.29.ffn_up.weight,        torch.float16 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.29.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.29.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.30.attn_q.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.30.attn_k.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.30.attn_v.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.30.attn_output.weight,   torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.30.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.30.ffn_down.weight,      torch.float16 --> Q8_0, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.30.ffn_up.weight,        torch.float16 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.30.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.30.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.31.attn_q.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.31.attn_k.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.31.attn_v.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.31.attn_output.weight,   torch.float16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.31.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.31.ffn_down.weight,      torch.float16 --> Q8_0, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.31.ffn_up.weight,        torch.float16 --> Q8_0, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.31.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.31.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:output_norm.weight,          torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:output.weight,               torch.float16 --> Q8_0, shape = {4096, 32000}\n",
      "INFO:hf-to-gguf:Set meta model\n",
      "INFO:hf-to-gguf:Set model parameters\n",
      "INFO:hf-to-gguf:gguf: context length = 2048\n",
      "INFO:hf-to-gguf:gguf: embedding length = 4096\n",
      "INFO:hf-to-gguf:gguf: feed forward length = 11008\n",
      "INFO:hf-to-gguf:gguf: head count = 32\n",
      "INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-06\n",
      "INFO:hf-to-gguf:gguf: file type = 7\n",
      "INFO:hf-to-gguf:Set model quantization version\n",
      "INFO:hf-to-gguf:Set model tokenizer\n",
      "INFO:gguf.vocab:Setting special token type bos to 1\n",
      "INFO:gguf.vocab:Setting special token type eos to 2\n",
      "INFO:gguf.vocab:Setting special token type pad to 0\n",
      "INFO:gguf.vocab:Setting add_bos_token to True\n",
      "INFO:gguf.vocab:Setting add_eos_token to False\n",
      "INFO:gguf.gguf_writer:Writing the following files:\n",
      "INFO:gguf.gguf_writer:vicuna-7b-v1.gguf: n_tensors = 291, total_size = 7.2G\n",
      "\n",
      "Writing:   0%|          | 0.00/7.16G [00:00<?, ?byte/s]\n",
      "Writing:   2%|         | 139M/7.16G [00:02<02:06, 55.3Mbyte/s]\n",
      "Writing:   2%|         | 157M/7.16G [00:02<02:04, 56.1Mbyte/s]\n",
      "Writing:   2%|         | 175M/7.16G [00:03<02:01, 57.5Mbyte/s]\n",
      "Writing:   3%|         | 193M/7.16G [00:03<02:01, 57.4Mbyte/s]\n",
      "Writing:   3%|         | 211M/7.16G [00:03<01:59, 58.1Mbyte/s]\n",
      "Writing:   4%|         | 258M/7.16G [00:04<01:58, 58.4Mbyte/s]\n",
      "Writing:   4%|         | 306M/7.16G [00:05<02:04, 54.9Mbyte/s]\n",
      "Writing:   5%|         | 354M/7.16G [00:06<01:56, 58.6Mbyte/s]\n",
      "Writing:   5%|         | 372M/7.16G [00:06<01:52, 60.2Mbyte/s]\n",
      "Writing:   5%|         | 390M/7.16G [00:06<01:49, 62.0Mbyte/s]\n",
      "Writing:   6%|         | 408M/7.16G [00:06<01:50, 61.2Mbyte/s]\n",
      "Writing:   6%|         | 426M/7.16G [00:07<01:56, 58.0Mbyte/s]\n",
      "Writing:   7%|         | 474M/7.16G [00:08<01:53, 58.8Mbyte/s]\n",
      "Writing:   7%|         | 521M/7.16G [00:09<01:55, 57.3Mbyte/s]\n",
      "Writing:   8%|         | 569M/7.16G [00:09<01:48, 60.7Mbyte/s]\n",
      "Writing:   8%|         | 587M/7.16G [00:10<01:48, 60.6Mbyte/s]\n",
      "Writing:   8%|         | 605M/7.16G [00:10<01:58, 55.4Mbyte/s]\n",
      "Writing:   9%|         | 623M/7.16G [00:10<02:02, 53.4Mbyte/s]\n",
      "Writing:   9%|         | 641M/7.16G [00:11<02:01, 53.5Mbyte/s]\n",
      "Writing:  10%|         | 689M/7.16G [00:12<01:56, 55.3Mbyte/s]\n",
      "Writing:  10%|         | 736M/7.16G [00:13<02:07, 50.4Mbyte/s]\n",
      "Writing:  11%|         | 784M/7.16G [00:14<02:08, 49.8Mbyte/s]\n",
      "Writing:  11%|         | 802M/7.16G [00:14<02:03, 51.6Mbyte/s]\n",
      "Writing:  11%|        | 820M/7.16G [00:14<01:56, 54.5Mbyte/s]\n",
      "Writing:  12%|        | 838M/7.16G [00:14<01:50, 57.3Mbyte/s]\n",
      "Writing:  12%|        | 856M/7.16G [00:15<01:45, 59.9Mbyte/s]\n",
      "Writing:  13%|        | 904M/7.16G [00:15<01:41, 61.9Mbyte/s]\n",
      "Writing:  13%|        | 952M/7.16G [00:16<01:43, 59.8Mbyte/s]\n",
      "Writing:  14%|        | 999M/7.16G [00:17<01:40, 61.3Mbyte/s]\n",
      "Writing:  14%|        | 1.02G/7.16G [00:17<01:41, 60.8Mbyte/s]\n",
      "Writing:  14%|        | 1.04G/7.16G [00:18<01:39, 61.4Mbyte/s]\n",
      "Writing:  15%|        | 1.05G/7.16G [00:18<01:40, 60.6Mbyte/s]\n",
      "Writing:  15%|        | 1.07G/7.16G [00:18<01:41, 60.2Mbyte/s]\n",
      "Writing:  16%|        | 1.12G/7.16G [00:19<01:43, 58.3Mbyte/s]\n",
      "Writing:  16%|        | 1.17G/7.16G [00:20<01:51, 53.6Mbyte/s]\n",
      "Writing:  17%|        | 1.21G/7.16G [00:21<01:44, 57.0Mbyte/s]\n",
      "Writing:  17%|        | 1.23G/7.16G [00:21<01:41, 58.5Mbyte/s]\n",
      "Writing:  17%|        | 1.25G/7.16G [00:21<01:37, 60.6Mbyte/s]\n",
      "Writing:  18%|        | 1.27G/7.16G [00:22<01:37, 60.3Mbyte/s]\n",
      "Writing:  18%|        | 1.29G/7.16G [00:22<01:35, 61.7Mbyte/s]\n",
      "Writing:  19%|        | 1.33G/7.16G [00:23<01:44, 55.6Mbyte/s]\n",
      "Writing:  19%|        | 1.38G/7.16G [00:24<01:49, 52.9Mbyte/s]\n",
      "Writing:  20%|        | 1.43G/7.16G [00:25<01:45, 54.2Mbyte/s]\n",
      "Writing:  20%|        | 1.45G/7.16G [00:25<01:47, 53.1Mbyte/s]\n",
      "Writing:  20%|        | 1.47G/7.16G [00:25<01:43, 55.2Mbyte/s]\n",
      "Writing:  21%|        | 1.48G/7.16G [00:26<01:42, 55.2Mbyte/s]\n",
      "Writing:  21%|        | 1.50G/7.16G [00:26<01:40, 56.2Mbyte/s]\n",
      "Writing:  22%|       | 1.55G/7.16G [00:27<01:42, 54.9Mbyte/s]\n",
      "Writing:  22%|       | 1.60G/7.16G [00:28<01:46, 52.2Mbyte/s]\n",
      "Writing:  23%|       | 1.64G/7.16G [00:29<01:42, 53.6Mbyte/s]\n",
      "Writing:  23%|       | 1.66G/7.16G [00:29<01:41, 54.1Mbyte/s]\n",
      "Writing:  23%|       | 1.68G/7.16G [00:29<01:37, 56.2Mbyte/s]\n",
      "Writing:  24%|       | 1.70G/7.16G [00:29<01:33, 58.7Mbyte/s]\n",
      "Writing:  24%|       | 1.72G/7.16G [00:30<01:28, 61.8Mbyte/s]\n",
      "Writing:  25%|       | 1.76G/7.16G [00:30<01:22, 65.0Mbyte/s]\n",
      "Writing:  25%|       | 1.81G/7.16G [00:31<01:27, 61.4Mbyte/s]\n",
      "Writing:  26%|       | 1.86G/7.16G [00:32<01:25, 62.1Mbyte/s]\n",
      "Writing:  26%|       | 1.88G/7.16G [00:32<01:24, 62.4Mbyte/s]\n",
      "Writing:  26%|       | 1.90G/7.16G [00:33<01:25, 61.5Mbyte/s]\n",
      "Writing:  27%|       | 1.91G/7.16G [00:33<01:27, 60.1Mbyte/s]\n",
      "Writing:  27%|       | 1.93G/7.16G [00:33<01:28, 58.9Mbyte/s]\n",
      "Writing:  28%|       | 1.98G/7.16G [00:34<01:32, 56.2Mbyte/s]\n",
      "Writing:  28%|       | 2.03G/7.16G [00:35<01:38, 52.2Mbyte/s]\n",
      "Writing:  29%|       | 2.07G/7.16G [00:36<01:32, 55.1Mbyte/s]\n",
      "Writing:  29%|       | 2.09G/7.16G [00:36<01:30, 56.0Mbyte/s]\n",
      "Writing:  29%|       | 2.11G/7.16G [00:36<01:27, 57.9Mbyte/s]\n",
      "Writing:  30%|       | 2.13G/7.16G [00:37<01:25, 59.1Mbyte/s]\n",
      "Writing:  30%|       | 2.15G/7.16G [00:37<01:21, 61.4Mbyte/s]\n",
      "Writing:  31%|       | 2.19G/7.16G [00:38<01:20, 61.7Mbyte/s]\n",
      "Writing:  31%|      | 2.24G/7.16G [00:39<01:24, 58.5Mbyte/s]\n",
      "Writing:  32%|      | 2.29G/7.16G [00:39<01:22, 58.8Mbyte/s]\n",
      "Writing:  32%|      | 2.31G/7.16G [00:40<01:23, 58.4Mbyte/s]\n",
      "Writing:  32%|      | 2.33G/7.16G [00:40<01:22, 58.5Mbyte/s]\n",
      "Writing:  33%|      | 2.34G/7.16G [00:40<01:22, 58.1Mbyte/s]\n",
      "Writing:  33%|      | 2.36G/7.16G [00:41<01:25, 56.0Mbyte/s]\n",
      "Writing:  34%|      | 2.41G/7.16G [00:42<01:28, 53.7Mbyte/s]\n",
      "Writing:  34%|      | 2.46G/7.16G [00:43<01:30, 51.7Mbyte/s]\n",
      "Writing:  35%|      | 2.50G/7.16G [00:43<01:26, 53.7Mbyte/s]\n",
      "Writing:  35%|      | 2.52G/7.16G [00:44<01:23, 55.4Mbyte/s]\n",
      "Writing:  35%|      | 2.54G/7.16G [00:44<01:19, 58.3Mbyte/s]\n",
      "Writing:  36%|      | 2.56G/7.16G [00:44<01:17, 59.7Mbyte/s]\n",
      "Writing:  36%|      | 2.58G/7.16G [00:44<01:13, 62.3Mbyte/s]\n",
      "Writing:  37%|      | 2.62G/7.16G [00:45<01:12, 62.3Mbyte/s]\n",
      "Writing:  37%|      | 2.67G/7.16G [00:46<01:21, 55.0Mbyte/s]\n",
      "Writing:  38%|      | 2.72G/7.16G [00:47<01:22, 54.1Mbyte/s]\n",
      "Writing:  38%|      | 2.74G/7.16G [00:48<01:25, 52.0Mbyte/s]\n",
      "Writing:  38%|      | 2.76G/7.16G [00:48<01:22, 53.7Mbyte/s]\n",
      "Writing:  39%|      | 2.77G/7.16G [00:48<01:20, 54.6Mbyte/s]\n",
      "Writing:  39%|      | 2.79G/7.16G [00:48<01:16, 57.2Mbyte/s]\n",
      "Writing:  40%|      | 2.84G/7.16G [00:49<01:10, 60.9Mbyte/s]\n",
      "Writing:  40%|      | 2.89G/7.16G [00:50<01:11, 59.6Mbyte/s]\n",
      "Writing:  41%|      | 2.93G/7.16G [00:51<01:08, 61.9Mbyte/s]\n",
      "Writing:  41%|      | 2.95G/7.16G [00:51<01:08, 61.6Mbyte/s]\n",
      "Writing:  41%|     | 2.97G/7.16G [00:51<01:07, 62.4Mbyte/s]\n",
      "Writing:  42%|     | 2.99G/7.16G [00:52<01:07, 61.7Mbyte/s]\n",
      "Writing:  42%|     | 3.01G/7.16G [00:52<01:08, 60.9Mbyte/s]\n",
      "Writing:  43%|     | 3.05G/7.16G [00:53<01:10, 57.8Mbyte/s]\n",
      "Writing:  43%|     | 3.10G/7.16G [00:54<01:16, 53.4Mbyte/s]\n",
      "Writing:  44%|     | 3.15G/7.16G [00:55<01:12, 55.6Mbyte/s]\n",
      "Writing:  44%|     | 3.17G/7.16G [00:55<01:09, 57.4Mbyte/s]\n",
      "Writing:  44%|     | 3.19G/7.16G [00:55<01:07, 59.3Mbyte/s]\n",
      "Writing:  45%|     | 3.20G/7.16G [00:55<01:05, 60.0Mbyte/s]\n",
      "Writing:  45%|     | 3.22G/7.16G [00:56<01:06, 59.6Mbyte/s]\n",
      "Writing:  46%|     | 3.27G/7.16G [00:57<01:06, 58.6Mbyte/s]\n",
      "Writing:  46%|     | 3.32G/7.16G [00:57<01:09, 55.6Mbyte/s]\n",
      "Writing:  47%|     | 3.37G/7.16G [00:58<01:11, 52.9Mbyte/s]\n",
      "Writing:  47%|     | 3.38G/7.16G [00:59<01:09, 54.2Mbyte/s]\n",
      "Writing:  47%|     | 3.40G/7.16G [00:59<01:07, 55.7Mbyte/s]\n",
      "Writing:  48%|     | 3.42G/7.16G [00:59<01:06, 56.0Mbyte/s]\n",
      "Writing:  48%|     | 3.44G/7.16G [01:00<01:09, 53.6Mbyte/s]\n",
      "Writing:  49%|     | 3.48G/7.16G [01:01<01:09, 52.8Mbyte/s]\n",
      "Writing:  49%|     | 3.53G/7.16G [01:01<01:06, 54.4Mbyte/s]\n",
      "Writing:  50%|     | 3.58G/7.16G [01:02<01:04, 55.5Mbyte/s]\n",
      "Writing:  50%|     | 3.60G/7.16G [01:03<01:01, 57.8Mbyte/s]\n",
      "Writing:  50%|     | 3.62G/7.16G [01:03<00:58, 60.3Mbyte/s]\n",
      "Writing:  51%|     | 3.63G/7.16G [01:03<00:57, 61.6Mbyte/s]\n",
      "Writing:  51%|     | 3.65G/7.16G [01:03<00:55, 62.9Mbyte/s]\n",
      "Writing:  52%|    | 3.70G/7.16G [01:04<00:57, 60.2Mbyte/s]\n",
      "Writing:  52%|    | 3.75G/7.16G [01:05<01:00, 56.1Mbyte/s]\n",
      "Writing:  53%|    | 3.80G/7.16G [01:06<00:58, 57.2Mbyte/s]\n",
      "Writing:  53%|    | 3.81G/7.16G [01:06<00:57, 57.8Mbyte/s]\n",
      "Writing:  54%|    | 3.83G/7.16G [01:06<00:55, 60.2Mbyte/s]\n",
      "Writing:  54%|    | 3.85G/7.16G [01:07<00:53, 62.0Mbyte/s]\n",
      "Writing:  54%|    | 3.87G/7.16G [01:07<00:51, 63.6Mbyte/s]\n",
      "Writing:  55%|    | 3.91G/7.16G [01:08<00:50, 64.8Mbyte/s]\n",
      "Writing:  55%|    | 3.96G/7.16G [01:09<00:52, 60.4Mbyte/s]\n",
      "Writing:  56%|    | 4.01G/7.16G [01:09<00:52, 60.2Mbyte/s]\n",
      "Writing:  56%|    | 4.03G/7.16G [01:10<00:51, 60.8Mbyte/s]\n",
      "Writing:  57%|    | 4.05G/7.16G [01:10<00:50, 62.2Mbyte/s]\n",
      "Writing:  57%|    | 4.06G/7.16G [01:10<00:49, 62.5Mbyte/s]\n",
      "Writing:  57%|    | 4.08G/7.16G [01:10<00:49, 62.7Mbyte/s]\n",
      "Writing:  58%|    | 4.13G/7.16G [01:11<00:51, 58.6Mbyte/s]\n",
      "Writing:  58%|    | 4.18G/7.16G [01:12<00:54, 54.8Mbyte/s]\n",
      "Writing:  59%|    | 4.23G/7.16G [01:13<00:56, 51.5Mbyte/s]\n",
      "Writing:  59%|    | 4.24G/7.16G [01:14<00:56, 51.7Mbyte/s]\n",
      "Writing:  60%|    | 4.26G/7.16G [01:14<00:55, 51.9Mbyte/s]\n",
      "Writing:  60%|    | 4.28G/7.16G [01:14<00:56, 51.3Mbyte/s]\n",
      "Writing:  60%|    | 4.30G/7.16G [01:15<00:53, 53.8Mbyte/s]\n",
      "Writing:  61%|    | 4.34G/7.16G [01:16<00:56, 49.9Mbyte/s]\n",
      "Writing:  61%|   | 4.39G/7.16G [01:17<00:55, 50.3Mbyte/s]\n",
      "Writing:  62%|   | 4.44G/7.16G [01:17<00:52, 51.5Mbyte/s]\n",
      "Writing:  62%|   | 4.46G/7.16G [01:18<00:51, 52.0Mbyte/s]\n",
      "Writing:  63%|   | 4.48G/7.16G [01:18<00:50, 53.1Mbyte/s]\n",
      "Writing:  63%|   | 4.49G/7.16G [01:18<00:52, 51.1Mbyte/s]\n",
      "Writing:  63%|   | 4.51G/7.16G [01:19<00:50, 52.3Mbyte/s]\n",
      "Writing:  64%|   | 4.56G/7.16G [01:20<00:47, 55.1Mbyte/s]\n",
      "Writing:  64%|   | 4.61G/7.16G [01:20<00:44, 57.2Mbyte/s]\n",
      "Writing:  65%|   | 4.66G/7.16G [01:21<00:41, 60.4Mbyte/s]\n",
      "Writing:  65%|   | 4.67G/7.16G [01:21<00:40, 61.3Mbyte/s]\n",
      "Writing:  66%|   | 4.69G/7.16G [01:22<00:42, 57.5Mbyte/s]\n",
      "Writing:  66%|   | 4.71G/7.16G [01:22<00:42, 57.6Mbyte/s]\n",
      "Writing:  66%|   | 4.73G/7.16G [01:22<00:42, 57.6Mbyte/s]\n",
      "Writing:  67%|   | 4.77G/7.16G [01:23<00:43, 54.9Mbyte/s]\n",
      "Writing:  67%|   | 4.82G/7.16G [01:24<00:44, 52.3Mbyte/s]\n",
      "Writing:  68%|   | 4.87G/7.16G [01:25<00:43, 52.4Mbyte/s]\n",
      "Writing:  68%|   | 4.89G/7.16G [01:26<00:44, 51.4Mbyte/s]\n",
      "Writing:  69%|   | 4.91G/7.16G [01:26<00:41, 54.9Mbyte/s]\n",
      "Writing:  69%|   | 4.92G/7.16G [01:26<00:38, 57.6Mbyte/s]\n",
      "Writing:  69%|   | 4.94G/7.16G [01:26<00:36, 60.1Mbyte/s]\n",
      "Writing:  70%|   | 4.99G/7.16G [01:27<00:34, 63.2Mbyte/s]\n",
      "Writing:  70%|   | 5.04G/7.16G [01:28<00:35, 60.5Mbyte/s]\n",
      "Writing:  71%|   | 5.09G/7.16G [01:29<00:33, 62.6Mbyte/s]\n",
      "Writing:  71%|  | 5.10G/7.16G [01:29<00:32, 62.6Mbyte/s]\n",
      "Writing:  72%|  | 5.12G/7.16G [01:29<00:32, 62.3Mbyte/s]\n",
      "Writing:  72%|  | 5.14G/7.16G [01:30<00:33, 60.5Mbyte/s]\n",
      "Writing:  72%|  | 5.16G/7.16G [01:30<00:33, 59.7Mbyte/s]\n",
      "Writing:  73%|  | 5.20G/7.16G [01:31<00:33, 57.6Mbyte/s]\n",
      "Writing:  73%|  | 5.25G/7.16G [01:32<00:34, 54.6Mbyte/s]\n",
      "Writing:  74%|  | 5.30G/7.16G [01:32<00:31, 58.2Mbyte/s]\n",
      "Writing:  74%|  | 5.32G/7.16G [01:33<00:42, 42.9Mbyte/s]\n",
      "Writing:  75%|  | 5.34G/7.16G [01:34<00:38, 47.0Mbyte/s]\n",
      "Writing:  75%|  | 5.35G/7.16G [01:34<00:35, 50.6Mbyte/s]\n",
      "Writing:  75%|  | 5.37G/7.16G [01:34<00:32, 54.3Mbyte/s]\n",
      "Writing:  76%|  | 5.42G/7.16G [01:35<00:29, 58.0Mbyte/s]\n",
      "Writing:  76%|  | 5.47G/7.16G [01:36<00:31, 54.3Mbyte/s]\n",
      "Writing:  77%|  | 5.52G/7.16G [01:37<00:29, 55.6Mbyte/s]\n",
      "Writing:  77%|  | 5.53G/7.16G [01:37<00:28, 56.9Mbyte/s]\n",
      "Writing:  78%|  | 5.55G/7.16G [01:37<00:27, 57.5Mbyte/s]\n",
      "Writing:  78%|  | 5.57G/7.16G [01:38<00:27, 57.6Mbyte/s]\n",
      "Writing:  78%|  | 5.59G/7.16G [01:38<00:26, 59.6Mbyte/s]\n",
      "Writing:  79%|  | 5.63G/7.16G [01:38<00:23, 64.1Mbyte/s]\n",
      "Writing:  79%|  | 5.68G/7.16G [01:39<00:26, 56.7Mbyte/s]\n",
      "Writing:  80%|  | 5.73G/7.16G [01:40<00:23, 60.0Mbyte/s]\n",
      "Writing:  80%|  | 5.75G/7.16G [01:40<00:22, 62.1Mbyte/s]\n",
      "Writing:  81%|  | 5.77G/7.16G [01:41<00:21, 65.0Mbyte/s]\n",
      "Writing:  81%|  | 5.78G/7.16G [01:41<00:20, 65.7Mbyte/s]\n",
      "Writing:  81%|  | 5.80G/7.16G [01:41<00:19, 68.0Mbyte/s]\n",
      "Writing:  82%| | 5.85G/7.16G [01:42<00:19, 66.5Mbyte/s]\n",
      "Writing:  82%| | 5.90G/7.16G [01:43<00:21, 58.9Mbyte/s]\n",
      "Writing:  83%| | 5.95G/7.16G [01:44<00:20, 58.6Mbyte/s]\n",
      "Writing:  83%| | 5.96G/7.16G [01:44<00:19, 60.8Mbyte/s]\n",
      "Writing:  84%| | 5.98G/7.16G [01:44<00:18, 62.9Mbyte/s]\n",
      "Writing:  84%| | 6.00G/7.16G [01:44<00:18, 63.9Mbyte/s]\n",
      "Writing:  84%| | 6.02G/7.16G [01:45<00:17, 63.6Mbyte/s]\n",
      "Writing:  85%| | 6.06G/7.16G [01:45<00:17, 61.4Mbyte/s]\n",
      "Writing:  85%| | 6.11G/7.16G [01:46<00:18, 57.3Mbyte/s]\n",
      "Writing:  86%| | 6.16G/7.16G [01:47<00:16, 61.0Mbyte/s]\n",
      "Writing:  86%| | 6.18G/7.16G [01:47<00:16, 58.9Mbyte/s]\n",
      "Writing:  87%| | 6.20G/7.16G [01:48<00:15, 60.4Mbyte/s]\n",
      "Writing:  87%| | 6.21G/7.16G [01:48<00:16, 57.6Mbyte/s]\n",
      "Writing:  87%| | 6.23G/7.16G [01:48<00:16, 57.7Mbyte/s]\n",
      "Writing:  88%| | 6.28G/7.16G [01:49<00:16, 53.3Mbyte/s]\n",
      "Writing:  88%| | 6.33G/7.16G [01:50<00:16, 51.0Mbyte/s]\n",
      "Writing:  89%| | 6.38G/7.16G [01:51<00:14, 52.7Mbyte/s]\n",
      "Writing:  89%| | 6.39G/7.16G [01:51<00:14, 53.6Mbyte/s]\n",
      "Writing:  90%| | 6.41G/7.16G [01:52<00:13, 55.0Mbyte/s]\n",
      "Writing:  90%| | 6.43G/7.16G [01:53<00:17, 42.7Mbyte/s]\n",
      "Writing:  90%| | 6.45G/7.16G [01:55<00:33, 21.0Mbyte/s]\n",
      "Writing:  91%| | 6.50G/7.16G [02:02<01:02, 10.6Mbyte/s]\n",
      "Writing:  91%|| 6.54G/7.16G [02:11<01:21, 7.57Mbyte/s]\n",
      "Writing:  92%|| 6.59G/7.16G [02:18<01:20, 7.06Mbyte/s]\n",
      "Writing:  92%|| 6.61G/7.16G [02:21<01:16, 7.19Mbyte/s]\n",
      "Writing:  93%|| 6.63G/7.16G [02:24<01:17, 6.93Mbyte/s]\n",
      "Writing:  93%|| 6.64G/7.16G [02:26<01:15, 6.83Mbyte/s]\n",
      "Writing:  93%|| 6.66G/7.16G [02:29<01:11, 6.98Mbyte/s]\n",
      "Writing:  94%|| 6.71G/7.16G [02:37<01:09, 6.51Mbyte/s]\n",
      "Writing:  94%|| 6.76G/7.16G [02:44<01:03, 6.37Mbyte/s]\n",
      "Writing:  95%|| 6.81G/7.16G [02:52<00:55, 6.42Mbyte/s]\n",
      "Writing:  95%|| 6.82G/7.16G [02:54<00:51, 6.52Mbyte/s]\n",
      "Writing:  96%|| 6.84G/7.16G [02:57<00:47, 6.73Mbyte/s]\n",
      "Writing:  96%|| 6.86G/7.16G [02:59<00:44, 6.71Mbyte/s]\n",
      "Writing:  96%|| 6.88G/7.16G [03:01<00:39, 7.09Mbyte/s]\n",
      "Writing:  97%|| 6.93G/7.16G [03:08<00:32, 7.19Mbyte/s]\n",
      "Writing:  97%|| 6.97G/7.16G [03:13<00:24, 7.73Mbyte/s]\n",
      "Writing:  98%|| 7.02G/7.16G [03:18<00:16, 8.33Mbyte/s]\n",
      "Writing:  98%|| 7.02G/7.16G [03:30<00:16, 8.33Mbyte/s]\n",
      "Writing: 100%|| 7.16G/7.16G [03:38<00:00, 7.59Mbyte/s]\n",
      "Writing: 100%|| 7.16G/7.16G [03:41<00:00, 32.3Mbyte/s]\n",
      "INFO:hf-to-gguf:Model successfully exported to vicuna-7b-v1.gguf\n"
     ]
    }
   ],
   "source": [
    "!python llama.cpp/convert_hf_to_gguf.py vicuna-hf \\\n",
    "  --outfile vicuna-7b-v1.gguf \\\n",
    "  --outtype q8_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46bd59f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wget in c:\\users\\dauddewan\\onedrive - symphonyai\\documents\\learning\\notebooks\\llm_llama_cpp_medusa_fastapi_dynamic_batching\\.venv\\lib\\site-packages (3.2)\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ee06f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!wget https://huggingface.co/TheBloke/vicuna-13B-v1.5-GGUF/blob/main/vicuna-13b-v1.5.Q2_K.gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bf112cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded file path: medusa_head\\medusa_lm_head.pt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "# Huggingface repository id\n",
    "model_id = \"FasterDecoding/medusa-vicuna-7b-v1.3\"\n",
    "\n",
    "# Name of the file you want to download\n",
    "filename = \"medusa_lm_head.pt\"  # adjust if filename is different\n",
    "\n",
    "# Download file\n",
    "file_path = hf_hub_download(\n",
    "    repo_id=model_id,\n",
    "    filename=filename,\n",
    "    revision=\"main\",  # branch or commit\n",
    "    local_dir=\"./medusa_head\",  # (optional) downloads to ./models/\n",
    ")\n",
    "\n",
    "print(f\"Downloaded file path: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553049d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
