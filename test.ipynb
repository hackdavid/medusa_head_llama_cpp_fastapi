{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8166e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(hidden_size)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_size * 4, hidden_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(torch.float32)\n",
    "        return x + self.mlp(self.norm(x))\n",
    "\n",
    "\n",
    "class CustomMedusaHead(nn.Module):\n",
    "    def __init__(self, hidden_size, vocab_size, num_pred_tokens=10, medusa_num_heads=1, medusa_num_layers=2):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_pred_tokens = num_pred_tokens  # âœ… fixed\n",
    "        self.medusa_num_heads = medusa_num_heads\n",
    "        self.medusa_num_layers = medusa_num_layers\n",
    "\n",
    "        self.projections = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                *[ResBlock(hidden_size) for _ in range(medusa_num_layers)],\n",
    "                nn.Linear(hidden_size, vocab_size, bias=False)\n",
    "            )\n",
    "            for _ in range(medusa_num_heads)\n",
    "        ])\n",
    "\n",
    "    def forward(self, input_ids: np.ndarray) -> np.ndarray:\n",
    "        if not isinstance(input_ids, np.ndarray):\n",
    "            raise ValueError(\"CustomMedusa expects input_ids as a numpy.ndarray\")\n",
    "        if input_ids.dtype != np.intc:\n",
    "            raise ValueError(f\"CustomMedusa expects dtype np.intc (int32), got {input_ids.dtype}\")\n",
    "\n",
    "        seq_len = input_ids.shape[-1]\n",
    "        hidden_states = torch.randn((seq_len, self.hidden_size), dtype=torch.float32)\n",
    "\n",
    "        logits = [proj(hidden_states) for proj in self.projections]\n",
    "        logits = torch.stack(logits, dim=0)\n",
    "        logits = logits[:, -1, :]  # last token\n",
    "\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        pred_tokens = torch.multinomial(probs, num_samples=self.num_pred_tokens, replacement=True)\n",
    "        return pred_tokens.flatten().cpu().numpy().astype(np.intc)\n",
    "\n",
    "    def __call__(self, input_ids: np.ndarray, /, **kwargs) -> np.ndarray:\n",
    "        return self.forward(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1adc25b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gguf = 'vicuna-7b-v1.gguf'\n",
    "medusa_path = 'medusa_lm_head.pt'\n",
    "hidden_size = 2048\n",
    "vocab_size = 32000\n",
    "d_type = torch.float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ee7a6c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Medusa head \n",
      "Medusa head is loaded successfully .....\n",
      "Loading model from ...\n",
      "Base Model is loaded successfully ..........\n",
      "Both Model loaded successfully! ........\n"
     ]
    }
   ],
   "source": [
    "# laoding mesusa head \n",
    "print(f'Loading Medusa head ')\n",
    "medusa_head = CustomMedusaHead(\n",
    "    hidden_size=hidden_size,     # depends on your base model\n",
    "    vocab_size=vocab_size,       # depends on your base model\n",
    "    medusa_num_heads=2,          # check config\n",
    "    medusa_num_layers=1          # check config\n",
    ")\n",
    "# Load the pretrained weights\n",
    "state_dict = torch.load(medusa_path)\n",
    "medusa_head.load_state_dict(state_dict,strict=False)\n",
    "print('Medusa head is loaded successfully .....')\n",
    "\n",
    "print(f\"Loading model from ...\")\n",
    "model = Llama(\n",
    "    model_path=model_gguf,\n",
    "    n_ctx=hidden_size,\n",
    "    use_mlock=True,      # lock into RAM to avoid swapping\n",
    "    use_mmap=True,       # memory map to load faster\n",
    "    logits_all=False,    # don't return logits unless necessary\n",
    "    seed=42,\n",
    "    verbose=False,\n",
    "    # draft_model=medusa_head \n",
    ")\n",
    "print(f'Base Model is loaded successfully ..........')\n",
    "\n",
    "print(\"Both Model loaded successfully! ........\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f33351aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<llama_cpp.llama_cpp.llama_context_params object at 0x0000023568F86A50>\n"
     ]
    }
   ],
   "source": [
    "print(model.context_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "af5e6a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"what is speculative decoding ?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "72417a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(\n",
    "            prompt,\n",
    "            max_tokens=50,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            echo=False,\n",
    "            stop=[\"</s>\"],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "51f76c7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'cmpl-f4b79cef-c198-4f53-a008-6923f8c2b86d',\n",
       " 'object': 'text_completion',\n",
       " 'created': 1745904244,\n",
       " 'model': 'vicuna-7b-v1.gguf',\n",
       " 'choices': [{'text': '\\n\\nSpeculative decoding is a technique used by the JIT (Just-In-Time) compiler to optimize the performance of a program. It involves analyzing the control flow and data dependencies of a program, and making intelligent predictions about',\n",
       "   'index': 0,\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'length'}],\n",
       " 'usage': {'prompt_tokens': 8, 'completion_tokens': 50, 'total_tokens': 58}}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf3fcb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
